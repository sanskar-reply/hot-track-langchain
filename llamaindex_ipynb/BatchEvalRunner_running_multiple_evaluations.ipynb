{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BatchEvalRunner - Running Multiple Evaluations\n",
        "\n",
        "The BatchEvalRunner class can be used to run a series of evaluations asynchronously. The async jobs are limited to a defined size of num_workers."
      ],
      "metadata": {
        "id": "jpCAXn2hGyeY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup"
      ],
      "metadata": {
        "id": "KgpL596OG7m0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# attach to the same event-loop\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "EpgTy28WGz8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELnMe9sRGuwd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
        "# openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    ServiceContext,\n",
        "    Response,\n",
        ")\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index.evaluation import (\n",
        "    FaithfulnessEvaluator,\n",
        "    RelevancyEvaluator,\n",
        "    CorrectnessEvaluator,\n",
        ")\n",
        "import pandas as pd\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 0)"
      ],
      "metadata": {
        "id": "-9y7o6yrHJCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using GPT-4 here for evaluation\n",
        "\n"
      ],
      "metadata": {
        "id": "jP9_Ik49HKyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gpt-4\n",
        "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
        "service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n",
        "\n",
        "faithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\n",
        "relevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)\n",
        "correctness_gpt4 = CorrectnessEvaluator(service_context=service_context_gpt4)"
      ],
      "metadata": {
        "id": "ooSh3dCFHMSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(\"./test_wiki_data/\").load_data()\n"
      ],
      "metadata": {
        "id": "hnlka0ANHNPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create vector index\n",
        "llm = OpenAI(temperature=0.3, model=\"gpt-3.5-turbo\")\n",
        "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=512)\n",
        "vector_index = VectorStoreIndex.from_documents(\n",
        "    documents, service_context=service_context\n",
        ")"
      ],
      "metadata": {
        "id": "nFfl2wvgHPWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question Generation\n",
        "To run evaluations in batch, you can create the runner and then call the .aevaluate_queries() function on a list of queries.\n",
        "\n",
        "First, we can generate some questions and then run evaluation on them."
      ],
      "metadata": {
        "id": "Cwn--vr8HQd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy datasets span-marker scikit-learn"
      ],
      "metadata": {
        "id": "QBAS5eB1HPPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.evaluation import DatasetGenerator\n",
        "\n",
        "dataset_generator = DatasetGenerator.from_documents(\n",
        "    documents, service_context=service_context\n",
        ")\n",
        "\n",
        "qas = dataset_generator.generate_dataset_from_nodes(num=3)"
      ],
      "metadata": {
        "id": "0oh9yqhHHVcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Batch Evaluation\n",
        "Now, we can run our batch evaluation!"
      ],
      "metadata": {
        "id": "s_CCxCwuHZBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.evaluation import BatchEvalRunner\n",
        "\n",
        "runner = BatchEvalRunner(\n",
        "    {\"faithfulness\": faithfulness_gpt4, \"relevancy\": relevancy_gpt4},\n",
        "    workers=8,\n",
        ")\n",
        "\n",
        "eval_results = await runner.aevaluate_queries(\n",
        "    vector_index.as_query_engine(), queries=qas.questions\n",
        ")\n",
        "\n",
        "# If we had ground-truth answers, we could also include the correctness evaluator like below.\n",
        "# The correctness evaluator depends on additional kwargs, which are passed in as a dictionary.\n",
        "# Each question is mapped to a set of kwargs\n",
        "#\n",
        "\n",
        "# runner = BatchEvalRunner(\n",
        "#     {\"correctness\": correctness_gpt4},\n",
        "#     workers=8,\n",
        "# )\n",
        "\n",
        "# eval_results = await runner.aevaluate_queries(\n",
        "#     vector_index.as_query_engine(),\n",
        "#     queries=qas.queries,\n",
        "#     reference=[qr[1] for qr in qas.qr_pairs],\n",
        "# )"
      ],
      "metadata": {
        "id": "AmdFjvESHbqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len([qr for qr in qas.qr_pairs]))"
      ],
      "metadata": {
        "id": "5yOrTH3NHfoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspecting Outputs"
      ],
      "metadata": {
        "id": "YE-bHNL0HhCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(eval_results.keys())\n",
        "\n",
        "print(eval_results[\"correctness\"][0].dict().keys())\n",
        "\n",
        "print(eval_results[\"correctness\"][0].passing)\n",
        "print(eval_results[\"correctness\"][0].response)\n",
        "print(eval_results[\"correctness\"][0].contexts)"
      ],
      "metadata": {
        "id": "7MvbsOj2HjG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reporting Total Scores"
      ],
      "metadata": {
        "id": "94cIulqlHld3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_eval_results(key, eval_results):\n",
        "    results = eval_results[key]\n",
        "    correct = 0\n",
        "    for result in results:\n",
        "        if result.passing:\n",
        "            correct += 1\n",
        "    score = correct / len(results)\n",
        "    print(f\"{key} Score: {score}\")\n",
        "    return score"
      ],
      "metadata": {
        "id": "aJBwCe4WHnG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correctness = get_eval_results(\"correctness\", eval_results)"
      ],
      "metadata": {
        "id": "vCTgNyetHo9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relevancy = get_eval_results(\"relevancy\", eval_results)"
      ],
      "metadata": {
        "id": "W58z-u_6HshV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}