{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WhwzhVwGNoL"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/examples/evaluation/RetryQuery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNxTnoAVGNoN"
      },
      "source": [
        "# Self Correcting Query Engines - Evaluation & Retry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC6o8d7GGNoN"
      },
      "source": [
        "In this notebook, we showcase several advanced, self-correcting query engines.  \n",
        "They leverage the latest LLM's ability to evaluate its own output, and then self-correct to give better responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4T7jA7mGNoO"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrthVp6GGNoO"
      },
      "outputs": [],
      "source": [
        "%pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYj8S0PvGNoP"
      },
      "outputs": [],
      "source": [
        "# Uncomment to add your OpenAI API key\n",
        "# import os\n",
        "# os.environ['OPENAI_API_KEY'] = \"INSERT OPENAI KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8BacashGNoP"
      },
      "outputs": [],
      "source": [
        "# Uncomment for debug level logging\n",
        "# import logging\n",
        "# import sys\n",
        "\n",
        "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
        "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-61CmNBGNoP"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PDyVPLpGNoP"
      },
      "source": [
        "First we ingest the document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhG0dXiqGNoQ"
      },
      "outputs": [],
      "source": [
        "from llama_index.indices.vector_store.base import VectorStoreIndex\n",
        "from llama_index.readers.file.base import SimpleDirectoryReader\n",
        "\n",
        "# Needed for running async functions in Jupyter Notebook\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_3o3_WZGNoQ"
      },
      "source": [
        "Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4rjYNWsGNoQ"
      },
      "outputs": [],
      "source": [
        "%mkdir -p 'data/paul_graham/'\n",
        "%wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0mAFJHlGNoQ"
      },
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzOWIFaqGNoQ"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query = \"What did the author do growing up?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoiQqFawGNoQ"
      },
      "source": [
        "Let's what the response from the default query engine looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1Dqz1hYGNoQ",
        "outputId": "a462622b-f299-4000-b419-bbca609db7e9"
      },
      "outputs": [],
      "source": [
        "base_query_engine = index.as_query_engine()\n",
        "response = base_query_engine.query(query)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4klDy9MGNoR"
      },
      "source": [
        "## Retry Query Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRcxSqCSGNoR"
      },
      "source": [
        "The retry query engine uses an evaluator to improve the response from a base query engine.  \n",
        "\n",
        "It does the following:\n",
        "1. first queries the base query engine, then\n",
        "2. use the evaluator to decided if the response passes.\n",
        "3. If the response passes, then return response,\n",
        "4. Otherwise, transform the original query with the evaluation result (query, response, and feedback) into a new query,\n",
        "5. Repeat up to max_retries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSCqQcbGGNoR",
        "outputId": "4ea8e576-7da7-41c9-fc59-f4188bb81bf1"
      },
      "outputs": [],
      "source": [
        "from llama_index.query_engine import RetryQueryEngine\n",
        "from llama_index.evaluation import RelevancyEvaluator\n",
        "\n",
        "query_response_evaluator = RelevancyEvaluator()\n",
        "retry_query_engine = RetryQueryEngine(\n",
        "    base_query_engine, query_response_evaluator\n",
        ")\n",
        "retry_response = retry_query_engine.query(query)\n",
        "print(retry_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kvEPhl4GNoR"
      },
      "source": [
        "## Retry Source Query Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it4sCVulGNoR"
      },
      "source": [
        "The Source Retry modifies the query source nodes by filtering the existing source nodes for the query based on llm node evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLUl1QBnGNoS",
        "outputId": "de8fff20-372b-4313-9e19-84f842eebd02"
      },
      "outputs": [],
      "source": [
        "from llama_index.query_engine import RetrySourceQueryEngine\n",
        "\n",
        "retry_source_query_engine = RetrySourceQueryEngine(\n",
        "    base_query_engine, query_response_evaluator\n",
        ")\n",
        "retry_source_response = retry_source_query_engine.query(query)\n",
        "print(retry_source_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXqQPPccGNoS"
      },
      "source": [
        "## Retry Guideline Query Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VQi7Ap7GNoS"
      },
      "source": [
        "This module tries to use guidelines to direct the evaluator's behavior. You can customize your own guidelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUjb853PGNoS"
      },
      "outputs": [],
      "source": [
        "from llama_index.evaluation.guideline import (\n",
        "    GuidelineEvaluator,\n",
        "    DEFAULT_GUIDELINES,\n",
        ")\n",
        "from llama_index.response.schema import Response\n",
        "from llama_index.indices.query.query_transform.feedback_transform import (\n",
        "    FeedbackQueryTransformation,\n",
        ")\n",
        "from llama_index.query_engine.retry_query_engine import (\n",
        "    RetryGuidelineQueryEngine,\n",
        ")\n",
        "\n",
        "# Guideline eval\n",
        "guideline_eval = GuidelineEvaluator(\n",
        "    guidelines=DEFAULT_GUIDELINES\n",
        "    + \"\\nThe response should not be overly long.\\n\"\n",
        "    \"The response should try to summarize where possible.\\n\"\n",
        ")  # just for example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLXRDY_PGNoS"
      },
      "source": [
        "Let's look like what happens under the hood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKxQmqKlGNoS",
        "outputId": "5bb8585b-8338-4b50-dbb7-03f73f09de29"
      },
      "outputs": [],
      "source": [
        "typed_response = (\n",
        "    response if isinstance(response, Response) else response.get_response()\n",
        ")\n",
        "eval = guideline_eval.evaluate_response(query, typed_response)\n",
        "print(f\"Guideline eval evaluation result: {eval.feedback}\")\n",
        "\n",
        "feedback_query_transform = FeedbackQueryTransformation(resynthesize_query=True)\n",
        "transformed_query = feedback_query_transform.run(query, {\"evaluation\": eval})\n",
        "print(f\"Transformed query: {transformed_query.query_str}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kIy304CGNoS"
      },
      "source": [
        "Now let's run the full query engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jboht9i4GNoS",
        "outputId": "f971795a-ecd8-4703-adc4-adafaa09610b"
      },
      "outputs": [],
      "source": [
        "retry_guideline_query_engine = RetryGuidelineQueryEngine(\n",
        "    base_query_engine, guideline_eval, resynthesize_query=True\n",
        ")\n",
        "retry_guideline_response = retry_guideline_query_engine.query(query)\n",
        "print(retry_guideline_response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
