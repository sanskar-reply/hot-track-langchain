{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9ad59b9d",
      "metadata": {
        "id": "9ad59b9d"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/examples/evaluation/answer_and_context_relevancy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c693f512-0033-4bca-9824-261701e4a4d4",
      "metadata": {
        "id": "c693f512-0033-4bca-9824-261701e4a4d4"
      },
      "source": [
        "# Answer Relevancy and Context Relevancy Evaluations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c69792a3-24a1-424b-afc0-c28b13f2cb42",
      "metadata": {
        "id": "c69792a3-24a1-424b-afc0-c28b13f2cb42"
      },
      "source": [
        "In this notebook, we demonstrate how to utilize the `AnswerRelevancyEvaluator` and `ContextRelevancyEvaluator` classes to get a measure on the relevancy of a generated answer and retrieved contexts, respectively, to a given user query. Both of these evaluators return a `score` that is between 0 and 1 as well as a generated `feedback` explaining the score. Note that, higher score means higher relevancy. In particular, we prompt the judge LLM to take a step-by-step approach in providing a relevancy score, asking it to answer the following two questions of a generated answer to a query for answer relevancy (for context relevancy these are slightly adjusted):\n",
        "\n",
        "1. Does the provided response match the subject matter of the user's query?\n",
        "2. Does the provided response attempt to address the focus or perspective on the subject matter taken on by the user's query?\n",
        "\n",
        "Each question is worth 1 point and so a perfect evaluation would yield a score of 2/2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00f5d108-6cad-4b5f-848d-6f4edeef2c61",
      "metadata": {
        "id": "00f5d108-6cad-4b5f-848d-6f4edeef2c61"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a7436d4-8cf2-444f-a776-4544f666ef3c",
      "metadata": {
        "id": "4a7436d4-8cf2-444f-a776-4544f666ef3c"
      },
      "outputs": [],
      "source": [
        "def displayify_df(df):\n",
        "    \"\"\"For pretty displaying DataFrame in a notebook.\"\"\"\n",
        "    display_df = df.style.set_properties(\n",
        "        **{\n",
        "            \"inline-size\": \"300px\",\n",
        "            \"overflow-wrap\": \"break-word\",\n",
        "        }\n",
        "    )\n",
        "    display(display_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55957d59-44ab-45d3-9716-fb1aeb69633e",
      "metadata": {
        "id": "55957d59-44ab-45d3-9716-fb1aeb69633e"
      },
      "source": [
        "### Download the dataset (`LabelledRagDataset`)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a228c9ab-96d5-4e4c-8b8c-febe8375b649",
      "metadata": {
        "id": "a228c9ab-96d5-4e4c-8b8c-febe8375b649"
      },
      "source": [
        "For this demonstration, we will use a llama-dataset provided through our [llama-hub](https://llamahub.ai)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df1997b3-d3fc-4a95-b139-09c1fb256021",
      "metadata": {
        "id": "df1997b3-d3fc-4a95-b139-09c1fb256021"
      },
      "outputs": [],
      "source": [
        "from llama_index.llama_dataset import download_llama_dataset\n",
        "from llama_index.llama_pack import download_llama_pack\n",
        "from llama_index import VectorStoreIndex\n",
        "\n",
        "# download and install dependencies for benchmark dataset\n",
        "rag_dataset, documents = download_llama_dataset(\n",
        "    \"EvaluatingLlmSurveyPaperDataset\", \"./data\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39e019d4-90ae-4e8c-91b6-7313493b4983",
      "metadata": {
        "id": "39e019d4-90ae-4e8c-91b6-7313493b4983",
        "outputId": "579b5f01-051f-4e76-a9d1-190d8f2518c5"
      },
      "outputs": [],
      "source": [
        "rag_dataset.to_pandas()[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a3ae70a-d1a2-4f86-909a-e9c6fb063930",
      "metadata": {
        "id": "3a3ae70a-d1a2-4f86-909a-e9c6fb063930"
      },
      "source": [
        "Next, we build a RAG over the same source documents used to created the `rag_dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4e6d215-1644-4da9-8f15-5c1c77ab35cf",
      "metadata": {
        "id": "a4e6d215-1644-4da9-8f15-5c1c77ab35cf"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_documents(documents=documents)\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88027888-102f-40df-a581-3047b35d4f11",
      "metadata": {
        "id": "88027888-102f-40df-a581-3047b35d4f11"
      },
      "source": [
        "With our RAG (i.e `query_engine`) defined, we can make predictions (i.e., generate responses to the query) with it over the `rag_dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d609c502-e2a7-4dfe-ba1d-5ee54fe59691",
      "metadata": {
        "id": "d609c502-e2a7-4dfe-ba1d-5ee54fe59691",
        "outputId": "e76f900c-1cd3-42bd-a981-ba7a95c8c3b8"
      },
      "outputs": [],
      "source": [
        "prediction_dataset = await rag_dataset.amake_predictions_with(\n",
        "    predictor=query_engine, batch_size=100, show_progress=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db779e26-dad1-4380-a131-b906339a936e",
      "metadata": {
        "id": "db779e26-dad1-4380-a131-b906339a936e"
      },
      "source": [
        "### Evaluating Answer and Context Relevancy Separately"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96344bde-30c7-416f-a650-58e79b5abaff",
      "metadata": {
        "id": "96344bde-30c7-416f-a650-58e79b5abaff"
      },
      "source": [
        "We first need to define our evaluators (i.e. `AnswerRelevancyEvaluator` & `ContextRelevancyEvaluator`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8fb6c86-b88a-4344-b390-a4decfa71061",
      "metadata": {
        "id": "b8fb6c86-b88a-4344-b390-a4decfa71061"
      },
      "outputs": [],
      "source": [
        "# instantiate the gpt-4 judges\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index import ServiceContext\n",
        "from llama_index.evaluation import (\n",
        "    AnswerRelevancyEvaluator,\n",
        "    ContextRelevancyEvaluator,\n",
        ")\n",
        "\n",
        "judges = {}\n",
        "\n",
        "judges[\"answer_relevancy\"] = AnswerRelevancyEvaluator(\n",
        "    service_context=ServiceContext.from_defaults(\n",
        "        llm=OpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
        "    )\n",
        ")\n",
        "\n",
        "judges[\"context_relevancy\"] = ContextRelevancyEvaluator(\n",
        "    service_context=ServiceContext.from_defaults(\n",
        "        llm=OpenAI(temperature=0, model=\"gpt-4\"),\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "650e5262-1fa0-411f-9f09-e2557f96116a",
      "metadata": {
        "id": "650e5262-1fa0-411f-9f09-e2557f96116a"
      },
      "source": [
        "Now, we can use our evaluator to make evaluations by looping through all of the <example, prediction> pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c5d02ce-c7f3-49b7-b397-b94b0aa3fc48",
      "metadata": {
        "id": "2c5d02ce-c7f3-49b7-b397-b94b0aa3fc48"
      },
      "outputs": [],
      "source": [
        "eval_tasks = []\n",
        "for example, prediction in zip(\n",
        "    rag_dataset.examples, prediction_dataset.predictions\n",
        "):\n",
        "    eval_tasks.append(\n",
        "        judges[\"answer_relevancy\"].aevaluate(\n",
        "            query=example.query,\n",
        "            response=prediction.response,\n",
        "            sleep_time_in_seconds=1.0,\n",
        "        )\n",
        "    )\n",
        "    eval_tasks.append(\n",
        "        judges[\"context_relevancy\"].aevaluate(\n",
        "            query=example.query,\n",
        "            contexts=prediction.contexts,\n",
        "            sleep_time_in_seconds=1.0,\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9155634d-58c7-4dac-ac16-93a8537ddef5",
      "metadata": {
        "id": "9155634d-58c7-4dac-ac16-93a8537ddef5",
        "outputId": "aa1f8486-ea5b-4e62-ab6b-451f49468df2"
      },
      "outputs": [],
      "source": [
        "eval_results1 = await tqdm_asyncio.gather(*eval_tasks[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a481eb08-1922-4e1c-bed1-459398c2e69c",
      "metadata": {
        "id": "a481eb08-1922-4e1c-bed1-459398c2e69c",
        "outputId": "ef08fe1e-b079-4e1d-8e16-b1715281b272"
      },
      "outputs": [],
      "source": [
        "eval_results2 = await tqdm_asyncio.gather(*eval_tasks[250:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7211e3f5-c52d-48da-98b0-7b4e5b9ce002",
      "metadata": {
        "id": "7211e3f5-c52d-48da-98b0-7b4e5b9ce002"
      },
      "outputs": [],
      "source": [
        "eval_results = eval_results1 + eval_results2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f56052e8-957f-4945-b910-24f70116ea79",
      "metadata": {
        "id": "f56052e8-957f-4945-b910-24f70116ea79"
      },
      "outputs": [],
      "source": [
        "evals = {\n",
        "    \"answer_relevancy\": eval_results[::2],\n",
        "    \"context_relevancy\": eval_results[1::2],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c31a4ac-5871-4e78-856d-5055f249598f",
      "metadata": {
        "id": "9c31a4ac-5871-4e78-856d-5055f249598f"
      },
      "source": [
        "### Taking a look at the evaluation results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d037782-b7fa-43f6-ad83-b80faad3606b",
      "metadata": {
        "id": "6d037782-b7fa-43f6-ad83-b80faad3606b"
      },
      "source": [
        "Here we use a utility function to convert the list of `EvaluationResult` objects into something more notebook friendly. This utility will provide two DataFrames, one deep one containing all of the evaluation results, and another one which aggregates via taking the mean of all the scores, per evaluation method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffa305d6-bba1-45ef-87a1-c4a2415fa54b",
      "metadata": {
        "id": "ffa305d6-bba1-45ef-87a1-c4a2415fa54b"
      },
      "outputs": [],
      "source": [
        "from llama_index.evaluation.notebook_utils import get_eval_results_df\n",
        "import pandas as pd\n",
        "\n",
        "deep_dfs = {}\n",
        "mean_dfs = {}\n",
        "for metric in evals.keys():\n",
        "    deep_df, mean_df = get_eval_results_df(\n",
        "        names=[\"baseline\"] * len(evals[metric]),\n",
        "        results_arr=evals[metric],\n",
        "        metric=metric,\n",
        "    )\n",
        "    deep_dfs[metric] = deep_df\n",
        "    mean_dfs[metric] = mean_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bf8b125-6406-41e9-afad-eb3786645576",
      "metadata": {
        "id": "5bf8b125-6406-41e9-afad-eb3786645576",
        "outputId": "44f7c14d-752f-474d-bb9c-84d4ded15def"
      },
      "outputs": [],
      "source": [
        "mean_scores_df = pd.concat(\n",
        "    [mdf.reset_index() for _, mdf in mean_dfs.items()],\n",
        "    axis=0,\n",
        "    ignore_index=True,\n",
        ")\n",
        "mean_scores_df = mean_scores_df.set_index(\"index\")\n",
        "mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])\n",
        "mean_scores_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc452d5a-a2f5-4fb6-b41e-ff61cc7cb810",
      "metadata": {
        "id": "cc452d5a-a2f5-4fb6-b41e-ff61cc7cb810"
      },
      "source": [
        "The above utility also provides the mean score across all of the evaluations in `mean_df`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52f8b67c-8501-4ad2-ba71-d20ed24dc041",
      "metadata": {
        "id": "52f8b67c-8501-4ad2-ba71-d20ed24dc041"
      },
      "source": [
        "We can get a look at the raw distribution of the scores by invoking `value_counts()` on the `deep_df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72768281-4fd2-480c-a1f2-5432606b0dfb",
      "metadata": {
        "id": "72768281-4fd2-480c-a1f2-5432606b0dfb",
        "outputId": "1b50e355-ba99-4957-8fcc-4241c6b9e077"
      },
      "outputs": [],
      "source": [
        "deep_dfs[\"answer_relevancy\"][\"scores\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43110a59-feba-42e7-aa77-d765a5844642",
      "metadata": {
        "id": "43110a59-feba-42e7-aa77-d765a5844642",
        "outputId": "fca866c3-197c-4082-ddea-ff0f8a393107"
      },
      "outputs": [],
      "source": [
        "deep_dfs[\"context_relevancy\"][\"scores\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4324ddbd-0b59-430a-89d5-c706bd55a184",
      "metadata": {
        "id": "4324ddbd-0b59-430a-89d5-c706bd55a184"
      },
      "source": [
        "It looks like for the most part, the default RAG does fairly well in terms of generating answers that are relevant to the query. Getting a closer look is made possible by viewing the records of any of the `deep_df`'s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a617540f-9eb7-47d7-8b96-7618236f1c69",
      "metadata": {
        "id": "a617540f-9eb7-47d7-8b96-7618236f1c69",
        "outputId": "02e53e4b-b573-4ba7-f07e-80fdd61be296"
      },
      "outputs": [],
      "source": [
        "displayify_df(deep_dfs[\"context_relevancy\"].head(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d2a0d5f-253d-433b-85b5-abbd7d95bbe5",
      "metadata": {
        "id": "5d2a0d5f-253d-433b-85b5-abbd7d95bbe5"
      },
      "source": [
        "And, of course you can apply any filters as you like. For example, if you want to look at the examples that yielded less than perfect results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8cfaa1e-720b-4753-b8cc-4c76b6f21e65",
      "metadata": {
        "id": "b8cfaa1e-720b-4753-b8cc-4c76b6f21e65",
        "outputId": "063b0893-f13a-4ce8-f4a0-98f0f113cd3c"
      },
      "outputs": [],
      "source": [
        "cond = deep_dfs[\"context_relevancy\"][\"scores\"] < 1\n",
        "displayify_df(deep_dfs[\"context_relevancy\"][cond].head(5))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llama_index_3.10",
      "language": "python",
      "name": "llama_index_3.10"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
