{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Query with the LangChain Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "# Run basic query with OpenAI wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Large language models are advanced artificial intelligence systems that are trained on vast amounts of text data and can generate human-like responses to prompts and questions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from IPython.display import Markdown\n",
    "client = OpenAI()\n",
    "\n",
    "def get_response(prompt_question):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and programming assistant\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "output = get_response(\"Explain large language models in one sentence\")\n",
    "Markdown(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and defining Schema for the chat messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import schema for chat messages and ChatOpenAI in order to query chatmodels GPT-3.5-turbo or GPT-4\n",
    "\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a simple example of how you can train a neural network using Python and Keras. This script will generate a dataset using sklearn, then it will train a neural network on this data.\n",
      "\n",
      "```python\n",
      "from sklearn.datasets import make_classification\n",
      "from tensorflow.keras.models import Sequential\n",
      "from tensorflow.keras.layers import Dense\n",
      "from tensorflow.keras.optimizers import Adam\n",
      "\n",
      "# Generate a binary classification dataset.\n",
      "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
      "\n",
      "# Define the neural network model.\n",
      "model = Sequential()\n",
      "model.add(Dense(25, input_dim=20, activation='relu'))\n",
      "model.add(Dense(1, activation='sigmoid'))\n",
      "\n",
      "# Compile the model.\n",
      "opt = Adam(learning_rate=0.01)\n",
      "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
      "\n",
      "# Fit the model on the dataset.\n",
      "model.fit(X, y, epochs=100, batch_size=10, verbose=0)\n",
      "\n",
      "# Evaluate the model.\n",
      "_, accuracy = model.evaluate(X, y, verbose=0)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "```\n",
      "\n",
      "This script creates a simple feedforward neural network with one hidden layer and trains it on a binary classification task. The data is generated using the `make_classification` function from sklearn. The model is trained for 100 epochs with a batch size of 10. The accuracy of the model is then printed out.\n",
      "\n",
      "Please note that this is a very basic example. In a real-world scenario, you would likely need to split your data into a training set and a test set, preprocess your data, tune your model's hyperparameters, etc.\n"
     ]
    }
   ],
   "source": [
    "chat = ChatOpenAI(model_name=\"gpt-4\",temperature=0.3)\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an expert data scientist\"),\n",
    "    HumanMessage(content=\"Write a Python script that trains a neural network on simulated data \")\n",
    "]\n",
    "response=chat(messages)\n",
    "\n",
    "print(response.content,end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import prompt and define PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert data scientist with an expertise in building deep learning models. \n",
    "Explain the concept of {concept} in a couple of lines\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"concept\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An autoencoder is a type of artificial neural network that is trained to reconstruct its input data. It consists of an encoder network that encodes the input data into a lower-dimensional representation and a decoder network that reconstructs the original data from the encoded representation. The autoencoder is trained to minimize the difference between the original input and the output generated by the decoder, effectively learning to compress and decompress the data.\n"
     ]
    }
   ],
   "source": [
    "output = get_response(prompt.format(concept=\"autoencoder\"))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and defining the LLMChain with language model and prompt as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An autoencoder is a type of neural network that is trained to reconstruct its input data. It consists of an encoder network that compresses the input data into a lower-dimensional representation, and a decoder network that reconstructs the original input from this representation. Autoencoders are commonly used for dimensionality reduction, feature extraction, and anomaly detection.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "print(chain.run(\"autoencoder\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"ml_concept\"],\n",
    "    template=\"Turn the concept description of {ml_concept} and explain it to me like I'm five in 500 words\",\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a sequential chain using the two chains above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mSelf-attention, also known as scaled dot-product attention, is a mechanism in deep learning models that allows the model to focus on different parts of the input sequence when making predictions. It computes attention weights for each element in the sequence by comparing it with all other elements, capturing the importance of each element in relation to others. This enables the model to effectively capture long-range dependencies and improve its ability to understand and generate coherent outputs.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mImagine you have a special power that allows you to pay attention to different things around you. Let's say you're in a classroom and the teacher is explaining a lesson. Normally, you would pay attention to the teacher's words and gestures to understand what they are saying. But sometimes, you might also want to pay attention to what your classmates are doing or what's written on the board.\n",
      "\n",
      "In deep learning models, there is a similar concept called self-attention. It's like a superpower that helps the model focus on different parts of a sequence of information, just like you can focus on different things in the classroom. This sequence of information could be words in a sentence, frames in a video, or any other ordered set of data.\n",
      "\n",
      "Now, let's dive a bit deeper into how self-attention works. Imagine you have a sentence like \"I love playing soccer.\" Each word in this sentence is like an element in the sequence. Self-attention helps the model understand the importance of each word in relation to the others.\n",
      "\n",
      "To do this, the model compares each word with all the other words in the sentence. It calculates something called attention weights, which tell us how much attention each word should receive. These weights are like a measure of importance. For example, in our sentence, the word \"soccer\" might have a high attention weight because it's the most important word for understanding the sentence.\n",
      "\n",
      "By calculating these attention weights, the model can effectively capture long-range dependencies. This means it can understand how different words in the sentence relate to each other, even if they are far apart. For example, in the sentence \"I love playing soccer,\" the word \"soccer\" is related to the word \"playing,\" even though they are not next to each other.\n",
      "\n",
      "This ability to capture long-range dependencies is really important because it helps the model generate coherent outputs. For example, if the model is trained to generate captions for images, it needs to understand the relationship between different objects in the image to generate a meaningful caption.\n",
      "\n",
      "To make the calculations easier, self-attention uses a technique called scaled dot-product attention. This technique scales down the dot product of two words' representations by dividing it by the square root of the dimension of the word vectors. This scaling helps to prevent the values from becoming too large or too small, making the attention weights more stable and reliable.\n",
      "\n",
      "In summary, self-attention is like a superpower that allows deep learning models to focus on different parts of a sequence of information. It calculates attention weights for each element in the sequence by comparing it with all the other elements. This helps the model understand the importance of each element in relation to others, capture long-range dependencies, and generate coherent outputs.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Imagine you have a special power that allows you to pay attention to different things around you. Let's say you're in a classroom and the teacher is explaining a lesson. Normally, you would pay attention to the teacher's words and gestures to understand what they are saying. But sometimes, you might also want to pay attention to what your classmates are doing or what's written on the board.\n",
      "\n",
      "In deep learning models, there is a similar concept called self-attention. It's like a superpower that helps the model focus on different parts of a sequence of information, just like you can focus on different things in the classroom. This sequence of information could be words in a sentence, frames in a video, or any other ordered set of data.\n",
      "\n",
      "Now, let's dive a bit deeper into how self-attention works. Imagine you have a sentence like \"I love playing soccer.\" Each word in this sentence is like an element in the sequence. Self-attention helps the model understand the importance of each word in relation to the others.\n",
      "\n",
      "To do this, the model compares each word with all the other words in the sentence. It calculates something called attention weights, which tell us how much attention each word should receive. These weights are like a measure of importance. For example, in our sentence, the word \"soccer\" might have a high attention weight because it's the most important word for understanding the sentence.\n",
      "\n",
      "By calculating these attention weights, the model can effectively capture long-range dependencies. This means it can understand how different words in the sentence relate to each other, even if they are far apart. For example, in the sentence \"I love playing soccer,\" the word \"soccer\" is related to the word \"playing,\" even though they are not next to each other.\n",
      "\n",
      "This ability to capture long-range dependencies is really important because it helps the model generate coherent outputs. For example, if the model is trained to generate captions for images, it needs to understand the relationship between different objects in the image to generate a meaningful caption.\n",
      "\n",
      "To make the calculations easier, self-attention uses a technique called scaled dot-product attention. This technique scales down the dot product of two words' representations by dividing it by the square root of the dimension of the word vectors. This scaling helps to prevent the values from becoming too large or too small, making the attention weights more stable and reliable.\n",
      "\n",
      "In summary, self-attention is like a superpower that allows deep learning models to focus on different parts of a sequence of information. It calculates attention weights for each element in the sequence by comparing it with all the other elements. This helps the model understand the importance of each element in relation to others, capture long-range dependencies, and generate coherent outputs.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "\n",
    "# Run the chain specifying only the input variable for the first chain.\n",
    "explanation = overall_chain.run(\"self-attention\")\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utility for splitting up texts and split up the explanation given above into document chunks\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap  = 0,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([explanation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Imagine you have a special power that allows you to pay attention to different things around you.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Individual text chunks can be accessed with \"page_content\"\n",
    "\n",
    "texts[0].page_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and instantiate OpenAI embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.008101751282596372, 0.002717129017224307, 0.028820459461437565, -0.02390176857761959, -0.021045340708388593, 0.006151567354502718, -0.026463587444766734, -0.04408890110172746, -0.011355261579149378, -0.019431394039774914, 0.018329812634706816, 0.009344233414536284, -0.006135555590770945, -0.0061771852451509585, 0.015050684447505569, -0.010836493472756278, 0.029870805085209184, 0.01676710361003481, -0.0008373944630363016, -0.008005683494173523, -0.017817449233806425, 0.007903211000257964, 0.0005531926999244641, -0.016062602769643525, -0.025695041412093552, -0.0016299571517768838, 0.012130211385992672, -0.008325910386905618, -0.00539903308556131, -0.01920083022997296, 0.04593340785485271, -0.01275785669179404, -0.005856957886757566, -0.0224799593484968, -0.016241930798149, -0.00017762609466894064, 0.011220765557770273, -0.024631886377336515, 0.02331255057345188, -0.0013617667449431837, 0.02712966005220176, 0.01488416676130811, -0.003999639394340407, 0.005943419548263947, -0.0033143529032579016, 0.0013641685095029496, -0.01688238458361319, -0.007486914922119125, -0.016651820773811235, 0.00146183782354495, 0.009100861125071507, 0.005437460387194969, -0.009728506430872875, -0.015473384765475818, -0.011476948189543064, -0.00866535139611584, -0.008806252122987654, -0.006186792303390022, 0.006455782949164338, -0.026899096242399806, -0.001850914019755099, -0.008658947621945726, 0.0011888440040070424, 0.007499724333104543, 0.002552211808908079, 0.006225219605023681, -0.026309878238232096, -0.0007004973173817156, -0.0008854285900783754, -0.009722101725380166, 0.02204445055086373, -0.0005587967008152602, 0.012059761488218064, -0.01854756703352335, 0.008678160807101258, -0.021852313111372836, -0.01894464946084536, 0.015511812067109478, 0.025669422590122715, 0.009004792405326062, 0.014576748348238838, -0.03583984531808916, -0.015588666670376795, 0.017983967851326478, 0.019623531479265804, 0.0027795732659636786, -0.014794502747055373, 0.01797115844034106, 0.004745768957789106, -0.0029204732943435454, 0.0037050303922565525, -0.01797115844034106, 0.023619968986521155, 0.010099970036224048, 0.013423930230551662, 0.032432623952356326, 0.0023296538809720107, 0.0161778856058671, -0.0022383891560073953, -0.009203333618987066, 0.008133774810059918, 0.01902150406411268, -0.045318571028714165, 0.001970999220944959, 0.001905352619459233, -0.013756966534269175, -0.005094817025238392, 0.002331255057345188, 0.021122195311655913, 0.002182349379930084, -0.030972388352922472, 0.02583594120764277, 0.029922042729150857, -0.032073971620635765, 0.01675429419904939, 0.00960041511498648, -0.012405607202920995, -0.00908164700859338, -0.02027679467571541, -0.025605377397840814, 0.018534757622537933, -0.0003314353019673226, -0.0029108664689351305, 0.01362887521838278, 0.010990201747968319, 0.02802629646943874, -0.010125587926872287, -0.003647389439806064, 0.004921894167886926, -0.015191585174377383, 0.032227680827170396, 0.023146031955931828, 0.008556474196707572, 0.014986639255223668, -0.017766213452509942, 0.002264007279486285, -0.0034552529316377685, 0.017625311794315535, -0.00675039241690949, 0.005610382778885137, 0.00998468813132307, 0.023274122340495626, -0.009792551623154774, -0.013692921341987276, -0.012059761488218064, 0.022275014360665684, 0.02294108696810071, 0.021032531297403175, 0.012444034504554653, 0.007378037722710857, 0.021045340708388593, 0.0010791660999968612, -0.015934512385079726, -0.012335157305146385, 0.002272012928521522, 0.017023284379162406, 0.009613224525971898, 0.0117587477806415, 0.002603448288696507, -0.020315222908671667, 0.03309869842243655, -0.016972048597865927, 0.02215973152444211, -0.02356873134257948, 0.020456122704220887, 0.025810324248317126, 0.002504177681866005, -0.005946621435349004, -0.014128430139620347, 0.01945701286174575, -0.01082368406177086, 0.027539551890509186, -0.05559146531927357, 0.01223268387990823, -0.0034296348081588788, 0.0038299188897352953, 0.004089302942931845, -0.007301183119443539, -0.01523001247601104, -0.017561266602033634, -0.003136626749617553, 0.007096237665951123, 0.012841116000554067, -0.006840055965500929, -0.013756966534269175, 0.0011616247041549754, 0.014025957645704789, 0.0006148364768925714, 0.003967616798199456, -0.015050684447505569, 0.011009415864446447, 0.019431394039774914, -0.006564660614233904, -0.02725775043676556, -0.6755003488214558, -0.004038066695974066, 0.00728196946862671, -0.01829138626439575, 0.007595792587188691, 0.03002451429174382, 0.007147474378570201, 0.01081727935627815, -0.0017212219931568241, 0.0045312169117189246, 0.013206175831735126, -0.01036896114765966, -0.011054247871572814, -0.003221487001920109, -0.012021334186584404, -0.011175934481966502, 0.0002848021751479478, -0.005142850919449817, 0.00567122608408198, 0.015447766874827577, -0.019687576671547705, 0.021941977125625574, -0.036634006447442795, 0.006330894451685595, -0.0054406622742800255, 0.006270051146488751, -0.004515205613648449, -0.031638459097712304, -0.0014162053446473177, 0.0372232244516105, -0.017087331434089498, -0.0038523348932984797, 0.027283369258736395, 0.03637782567831519, 0.05400313933527592, 0.015025066556857328, -0.003615366610834465, 0.042551809036381096, -0.023607159575535736, 0.0233894051767192, 0.007499724333104543, -0.001262496254528006, 0.014051575536353028, -0.01311010711198968, -0.005232514468041255, -0.012296730003512727, -0.016651820773811235, 0.006484603658220231, -0.007647028834146471, -0.031023624134218955, -0.004316664399987445, 0.003032552846498168, -0.017996775399666706, 0.01785587560411749, 0.02661729665130137, -0.0031510371041454997, 0.016562158622203692, 0.019034313475098097, 0.010977393268305496, 0.004902680517070096, -0.003452050578891414, 0.025426051231980534, -0.020532977307488202, -0.0144230391417042, 0.00046833247672573963, 0.028231241457269858, -0.0017388344676004764, -0.0022383891560073953, -0.025016159393673108, -0.021122195311655913, 0.020981295516106693, 0.024234803950014507, -0.016408449415669053, 0.004486384904592556, 0.0052965601259844525, 0.02222377671672401, 0.022108495743145627, -0.006218814899530972, -0.0014514304099499467, 0.009325020229380752, 0.0003748661342480051, -0.015524621478094896, -0.04257742785835193, -0.004169359898945518, 0.018534757622537933, -0.013961911522100292, -0.023888959166634173, -0.021275904518190548, 0.010637952259095274, -0.011720320479007841, -0.0037274461629890876, 0.007179496974711151, -0.01031132066087047, -0.03496882399753263, -0.01311010711198968, 0.0038043007662564056, -0.036172878827838886, 0.037479408946028483, 0.00853085630605933, 0.00412452789181915, -0.017420366806484417, -0.018278576853410333, -0.003980425743523577, -0.008646138210960308, 0.007378037722710857, 0.005767294338166127, -0.0016291565635902952, 0.014640793540520737, 0.03712075288901754, -0.018893413679548877, 0.007384442428203566, -0.015281248257307523, 0.010093565330731339, 0.007550960580062323, -0.0035993553127639903, -0.027078424270905278, 0.004393519003254763, 0.012648979492385772, -0.0041469438953823335, -0.013308648325650685, 0.030383170348754766, 0.008018492905158941, 0.020981295516106693, -0.011323238983008427, -0.012117401975007254, 0.016049793358658106, 0.002790781267745271, -3.977823962884404e-05, -0.010804469945292732, -0.008415574401158353, 0.011015820569939156, 0.003211880176511694, -0.004050875641298186, -0.006660728402656754, -0.0081209653990745, -0.003136626749617553, -0.013052466625200489, -0.018342622045692234, -0.004905882869816451, -0.02628425941626126, -0.00943389742878902, -0.007621410477836932, 0.00010547488151421077, 0.0022351868032610407, -0.010586715546476196, 0.002701117486323183, -0.028589897514280805, -0.0037082325121722583, -0.022057258099203954, -0.020827586309572058, -0.005049985018112024, -0.027360223862003714, -0.022633668555031437, 0.01275785669179404, 0.003109407449765486, -0.005543135233856882, -0.02093005787216502, -0.01725384818896436, -0.04321787978117093, -0.015703948575277772, 0.013846629617199315, -0.00140499745928105, -0.0011784365904120389, -0.011777961897119627, -0.008556474196707572, -0.01303965721421507, -0.0033015439579337814, 0.014692030253139815, -0.031228569122050073, -0.02628425941626126, 0.003954807387214038, -0.013897866329818393, -0.004899478164323742, 0.019213639640958378, -0.011995716295936163, 0.010567502361320666, -0.004153348600875043, -0.00898557922017053, 0.002062263945909575, -0.02003342145492804, 0.0028708379909282945, 0.014256521455506743, -0.02671976821389433, 0.0006136356528203507, 0.03758188050862145, -0.012975612021933172, 0.02043050388225005, 0.007755906033554739, -0.0013017241443482536, 0.02441413197851998, 0.009760529027013826, 0.027565168849834832, -0.005805721639799787, 0.0058537555340112115, -0.01356483002610088, 0.013052466625200489, 0.01134245216816396, -0.010055138029097679, 0.0027155278408511297, 0.03258633315889096, 0.0070578103643174645, 0.0008910325909691714, 0.025912795810910088, -0.019367348847493013, 0.005760889632673418, -0.004089302942931845, 9.846990950454687e-05, -0.03345735075415711, 0.009158501611860699, 0.009997497542308489, -0.0019453810974660692, 0.008710183403242209, 0.014794502747055373, -0.012898757418665854, 0.020955676694135856, 0.033867244455109725, 0.0144230391417042, 0.0011207956379615503, 0.002505778858239182, -0.0054054373253927205, 0.03391847837376102, -0.0063917377568824384, 0.009485134141408098, -0.004159753306367752, 0.01037536585315237, 0.0013617667449431837, 0.001956589099247661, 0.02268490433632792, -0.0053862236745758916, -0.036044788443275085, 0.010913348076023597, -0.009863001520929384, 0.013552020615115462, 0.022390295334244064, -0.018983075831156423, -0.005888980948559813, 0.0076854561357801296, -0.017574076013019052, 0.019085549256394576, 0.005543135233856882, 0.016485304018936373, 0.011540993381824963, 0.017343512203217098, -0.01725384818896436, 0.00140499745928105, -0.00245614378765458, 0.014845739459674451, -0.013001229912581413, -0.02043050388225005, -0.0010663570382574163, -0.034379605993364926, -0.012655384197878481, 0.0077687149788788585, -0.009344233414536284, 0.007480510682287714, 0.009241760920620725, 0.0052677398825898576, 0.01784306805577726, 0.006641514751839924, 0.038504135747829266, 0.0048034099102395944, 0.028000677647467904, 0.014730457554773474, -0.008185010591356399, -0.0017484412930088911, 0.009369852236507121, 0.01945701286174575, -0.008844679424621313, -0.013885056918832975, 0.014871357350322691, -0.0077943333351883975, -0.010605929662954323, -0.014845739459674451, -0.018188912839157596, 0.034097806402266485, -0.00654224461067072, -0.02641235166347025, 0.0019485833337970994, 0.01088772925405276, 0.01752284023172257, 0.018829366624621785, -0.01971319549351854, 0.000728116911327077, -0.0022656084558594623, 0.011092675173206473, -0.02080196748760122, -0.02044331329323547, 0.0019854095754729055, 0.007294778879612128, -0.0020062241698322633, -0.00040248572819336645, 0.0004263026446677562, 0.006209207841291908, -0.0023072378774088268, -0.013539212135452639, -0.008325910386905618, 0.011502566080191306, -0.003977223390777222, -0.0036666030906228934, -0.014410229730718782, -0.0065902785048821445, -0.001633159388107914, -0.005972239791658543, -0.017458795039440672, 0.01443584855268962, 0.02295389637908613, -0.03002451429174382, -0.007390847133696275, -0.029409679328250466, -0.02705280544893444, -0.0017388344676004764, -0.012347965784809208, 0.010266488653744101, -0.012584934300103871, 0.017458795039440672, -0.011316834277515718, -0.004102112353917263, 0.01276426139728675, 0.03537871769848525, 0.0226720949253425, -0.010766042643659074, -0.04198820985418422, 0.004502396202663031, 0.017343512203217098, 0.09683674796275545, -0.00678241501305044, -0.009696483834731925, 0.012738643506638508, -0.015447766874827577, -0.03594231688068212, -0.014090002837986688, -0.014141239550605766, 0.02158332293125982, 0.0034840734078630128, 0.0007793532747001809, -0.02828247910121153, 0.016677439595782072, -0.010080756851068516, -0.0040925052956781996, 0.0037178393375806727, -0.014743266965758893, 0.0004555233858824637, -0.014666412362491573, 0.006961742110233316, -0.013244602202046188, 0.0062860624445592265, 0.022134112702471273, 0.024119522976436127, 0.008787038006509526, 0.0034968825860177822, 0.006141960296263654, 0.02133994971047245, 0.00947232473042268, -0.03202273397669409, 0.0049635238222669395, 0.0031702505221316802, 0.006010667093292202, -0.009478729435915389, 0.000735322030383388, -0.015345294380912018, -0.008069728686455422, 0.01713856721538598, 0.010983797973798205, -0.018137677057861117, 0.021749841548779875, 0.03576298885217665, 0.004380710057930642, -0.018790940254310724, 0.01733070279223168, -0.011944479583317087, -0.012444034504554653, 0.026361114019528578, -0.027334605040032878, 0.004713746361648155, 0.014768884856407132, 0.011195147667122032, 0.0013201372069784945, -0.011316834277515718, 0.01662620381448559, 0.008345124503383745, 0.012937184720299512, -0.004098910001170909, -0.013936293631452051, -0.013257411613031607, -0.012514484402329263, 0.002753955142484789, -0.0023936993060845596, -0.021660177534527137, -0.007608601532512811, -0.018009584810652124, 0.004044471401466775, -0.015204393654040204, -0.027078424270905278, 0.0003042159730114245, 0.0073268014757530775, -0.0022928277557115293, -0.04193697221024255, -0.005299762478730807, 0.03461017166581207, 0.004377507705184288, 0.008441192291806595, -0.006558255908741195, -0.012988420501595994, -0.0017932730673046104, 0.010932561261179127, -0.029845188125883538, 0.008076133391948131, -0.003977223390777222, -0.02164736812354172, 0.013692921341987276, 0.010087161556561225, -0.008895915205917794, -0.0037050303922565525, 0.005962633199080777, 0.017817449233806425, 0.013103702406496972, 0.06614615920093142, -0.007442083380654055, 0.005888980948559813, 0.012476057100695603, 0.0010031120849161316, 0.007999278788680814, -0.0068848879726272965, -0.017112948393415144, 0.013808202315565656, -0.03860660731042223, -0.015524621478094896, -0.005937014842771238, -0.0031702505221316802, 0.010522670354194297, 0.0003914779316660838, 0.01362887521838278, -0.017305085832906034, -0.009888620342900221, 0.012123806680499963, -0.03427713443077196, 0.011502566080191306, 0.0032471051253989986, 0.018240148620454078, 0.02446536775981646, 0.010452220456419688, 0.02789820608487494, -0.007480510682287714, -0.048162193212250126, 0.012303134709005436, -0.031023624134218955, 0.021826696152047194, 0.0368389532979191, 0.005453471685265444, -0.018637231047776086, 0.009068838528930558, 0.00654224461067072, 0.015114730571110064, 0.010042329549434856, -0.00892153402788863, 0.013513593313481802, -0.025016159393673108, -0.04634330155580533, -0.038657844954363904, 0.002492969912915062, -0.01894464946084536, -0.0004451160013912914, -0.026386732841499415, -0.01443584855268962, 0.003615366610834465, -0.014051575536353028, -0.0049731304148447055, -0.02391457798860501, 0.04567722708572511, -0.03525062358863107, -0.02641235166347025, 0.006647919457332633, -0.031612844001031856, 0.029204732477774158, 0.021416804313739764, -0.0036281757889892346, -0.01651092097826202, -0.014640793540520737, -0.001987010519015434, -0.031074859915515434, -0.02215973152444211, -0.013680111931001857, 0.006068308045742691, 0.006910505863275537, 0.014512703155956938, 0.0023344571772608937, 0.011329642757178541, 0.023888959166634173, -0.009606819820479189, 0.000793763512812803, 0.0062860624445592265, -0.02242872356720032, -0.01490978465195635, 0.02700156966763796, 0.014256521455506743, 0.016370021182712798, -0.0031958688784412192, -0.02189074134432909, -0.003426432455412524, 0.0052165031699707804, -0.008620520320312068, 0.0029973278976108634, -0.0015474985476187698, -0.011771557191626918, 0.0035257030622430265, 0.017804639822821006, -0.01905993043442374, 0.015076303269476406, -0.01034334325701142, -0.01255931640945563, 0.02312041313396099, 0.006516626254361181, 0.002653083592111758, 0.004367900646945224, 0.02661729665130137, -0.015178775763391964, 0.04467811910589517, 0.005949823788095359, 0.03627535411572223, -0.01262976630723024, -0.006013869446038557, -0.00497633276759106, -0.00036285760248748667, 0.013334266216298926, 0.05354201171567201, 0.01881655907628156, -0.006904101157782828, 0.015345294380912018, -0.012488866511681022, 0.012450439210047362, 0.007045001418993344, -0.007205114865359391, -0.00383312124248165, -0.020135894880166195, -0.020251175853744575, -0.020135894880166195, -0.0058729691848280405, 0.006539042257924365, 0.0035481188329755616, -0.00808253809744084, -0.014807312158040792, 0.018893413679548877, -0.015703948575277772, -0.01958510324630955, -0.00541184203088543, -0.007198710625527981, 0.02725775043676556, -0.016395640004683635, 0.017625311794315535, 0.010253679242758683, -0.010778852054644492, -0.004902680517070096, 0.011912456987176136, -0.01220066128376728, 0.0013753763948692172, 0.019405777080449268, 0.0037370529883975026, -0.019623531479265804, -0.01584484837082699, 0.007467701271302296, 0.012604148416581999, -0.00821703318749735, -0.02255681395176412, -0.01050345623771617, 0.019674767260562286, 0.00431346204724109, -0.008927937802058744, -0.0003514495118669097, 0.012245493290893649, 0.03496882399753263, 0.015575857259391377, -0.004419137359564302, -0.005882576243067104, -0.007205114865359391, -0.028103151072706056, -0.00804411079580718, -0.02563099621981165, 0.027360223862003714, 0.0038555372460448343, 0.025157059189222324, 0.0011288014034121123, 0.01030491595537776, -0.008063323980962713, 0.004210989553325531, -0.01027289335923681, 0.012732238801145799, -0.00805051550129989, 0.004063685052283605, 0.008204224707834527, -0.01830419381273598, 0.01178436567128974, -0.015601476081362213, 0.008486024298932963, 0.02036645868996815, -0.004002841747086761, -0.01784306805577726, 0.0016619798643331583, 0.012059761488218064, 0.013821011726551074, 0.002011027698951795, -0.012482461806188313, -0.006987360466542855, 0.01894464946084536, -0.03061373415855672, 0.015819229548856152, 0.005033973720041549, -0.03168969674165398, 0.0022415912759231006, 0.0020462526478390998, -0.003103002977103426, -0.03550680808304905, -0.003436039280820939, 0.012930780014806803, -0.005700046793137873, -0.02866675025490293, 0.013769775945254593, 0.019687576671547705, 0.012264707407371777, 0.01046502893608251, 0.016241930798149, 0.01175234307514879, 0.03627535411572223, -0.01651092097826202, -0.0022752152812678767, -0.0002599845816479845, 0.011092675173206473, -0.023940194947930652, 0.01859880281481983, -0.0012368781310491163, -0.02401704955119797, 0.014090002837986688, -0.032740042365425596, -0.03291937039393107, 0.002324850351852479, 0.006558255908741195, 0.014845739459674451, 0.004710544008901801, 0.007647028834146471, -0.01348797542283356, 0.0012224678929364941, 0.004166157546199163, -0.01886779485757804, -0.017241040640624136, -0.02666853243259785, -0.019316113066196534, 0.00027759708519546785, 0.0469069007380022, -0.027488314246567513, -0.003407218804595695, 0.002717129017224307, 0.009702888540224634, -0.0031990709983569245, -0.029409679328250466, -0.016715867828738327, -0.04250057139243942, 0.0003568533366072274, -0.004403126061493827, -0.015447766874827577, 0.02274894952860982, -0.009536369922704579, -0.007871187472794418, 0.015063493858490987, 0.02376086878207037, -0.020084659098869712, -0.01571675798626319, 0.0372232244516105, 0.021365568532443285, 0.014397421251055961, -0.012809093404413116, -0.0015651111384777465, -0.02062264132174094, 0.008454001702792013, -0.0070578103643174645, -0.020340839867997312, -0.023632776534861382, 0.020942867283150438, 0.0017356321148541218, 0.005642405840687385, -0.014025957645704789, -0.0069489331649091966, -0.02268490433632792, -0.03399533483967353, -0.006641514751839924, 7.195108065999824e-05, 0.005946621435349004, 0.015447766874827577, 0.001289715554380073, 0.008492429004425673, -0.0012512883691617384, -0.017100138982429725, -0.028794842502111923, 0.003669805210538599, -0.014999448666209086, -0.006705560409783122, 0.005270941769674914, 0.0017660537674525434, -0.021391185491768928, -0.014358993949422302, 0.009376256010677235, -0.03384162563313889, 0.01900869465312726, 0.02080196748760122, -0.013449548121199903, -0.010593120251968905, 0.013795393835902834, 0.01446146644333786, 0.0006272453027463842, 0.0025538129852812562, 0.007864783698624304, -0.02615616903169746, -0.0013041257924926951, -0.00569043973489881, 0.001919762857571855, -0.022006022317907475, 0.03609602608721676, 0.035891079236740446, -0.01397472093308571, 0.04365339230409439, -0.03135665950661387, 0.001545897487660917, 0.008306697201750085, -0.004739364717957694, 0.0006548648966917455, 0.002606650408612213, -0.010535479765179715, 0.026309878238232096, 0.022710523158298756, -0.015063493858490987, -0.028461805267071812, 0.009068838528930558, 0.011201552372614741, 0.00862692409448218, 0.011105483652869296, -0.028513042911013486, 0.010324129140533293, -0.004588857864169413, -0.003288734779779012, -0.010682784266221643, 0.01953386746501307, -0.017292276421920615, -0.009094456419578798, -0.03640344450028603, 0.0061131400528690585, -0.011150315659995665, -0.025426051231980534, -0.010189634050476784, -0.02848742408904265, -0.0008950354154867901, -0.021288713929175966, -0.01314212970813063, 0.0077302876772452, 0.00018573186088324836, -0.0010423399747363795, 0.03368791642660425, 0.0024369301368377507, -0.019418586491434686, -0.017176993585697044, 0.016895193994598608, -0.0054310556817022595, 0.017164186037356817, 0.2309735768430339, 0.0059274077845321745, -0.014090002837986688, 0.04342282663164724, -0.013577639437086298, 0.04342282663164724, 0.020148704291151613, -0.0017420367039315064, -0.01446146644333786, 0.003941998441889918, -0.007506128572935955, -0.023709631138128698, -0.007109047076936542, 0.005658417138757859, 0.004015650692410881, -0.027155278874172597, -0.032740042365425596, -0.01523001247601104, -0.004764982608605935, -0.02815438685400254, 0.012809093404413116, -0.00985659774675927, -0.014538321046605178, -0.025080204585955005, -0.0050724010216752075, 0.012713025615990267, 0.004393519003254763, -0.017036093790147824, 0.03494320517556179, 0.004729757659718631, -0.009062433823437849, -0.0018317003689382694, -0.0004507199731782563, 0.00853726101155204, -0.003359184677553621, 0.004367900646945224, 0.00728196946862671, -0.012424820388076525, 0.021993212906922056, 0.0059081941337153455, 0.0008413973457615826, 0.005306167184223516, 0.015678329753306935, 0.0035160962368346116, 0.023786485741396017, 0.012578529594611162, -0.011624252690584992, -0.017164186037356817, 0.028256860279240695, 0.008902319911410503, -0.010170419933998656, -0.004835432972041843, 0.01352640272446722, 0.028794842502111923, -0.014410229730718782, 0.018509140663212287, 0.021352759121457867, 0.01034334325701142, 0.013436738710214485, 0.02301794157136803, -0.0012496871927885611, 0.017023284379162406, -0.01572956739724861, 0.01854756703352335, 0.008357933914369163, 0.022262204949680266, 0.0045600371551135195, -0.012360775195794626, 0.012975612021933172, -0.008018492905158941, 0.008383551805017405, -0.019252067873914633, -0.007928828890906204, -0.021391185491768928, -0.011419306771431277, -0.011553801861487786, 0.014000338823733952, 0.00010717608775496496, 0.036941424860512065, 0.028051915291409577, -0.008005683494173523, 0.004877062160760558, -0.012706620910497558, 0.0033175552560042562, -0.009805361034140193, -0.0369670436824829, 0.00610033064188364, -0.01214302079697809, -0.02158332293125982, -0.004790600964915474, 0.0006028279451320529, -0.03599355079933341, -0.0172154218186533, -0.004707341656155446, -0.008389956510510114, -0.00032843315447527745, 0.0054951008739841595, 0.009331424934873461, -0.019854095289067758, -0.005418246736378139, 0.017676549438257208, -0.010522670354194297, 0.003746659813805917, 0.00904962441245243, 0.002853225516484642, 0.019636340890251222, -0.0020542582968743374, 0.012732238801145799, 0.0207251128843339, -0.020776350528275575, 0.007237137927161639, -0.02912787787450684, -0.0007477308562372009, 0.00011908453871620206, -0.0037754802900311613, 0.017241040640624136, 0.005943419548263947, 0.014704839664125233, 0.0038843574894394293, -0.006209207841291908, 0.0001391988221391127, -0.010042329549434856, 0.017343512203217098, -0.004185371197015993, 0.019700386082533123, -0.004297450749170616, -0.02035364927898273, 0.028308096060537177, 0.011163125070981083, -0.00862692409448218, 0.0023376595300072483, -0.013910675740803811, 0.03899088218940401, -0.004479980664761145, -7.749251558983313e-06, 0.008748610704875866, 0.01255931640945563, -0.012552911703962921, -0.012335157305146385, 0.0035897484873555754, 0.0009614826051591048, -0.012162233982133622, 0.010996606453461028, 0.014922594062941769, 0.02113500472264133, -0.015703948575277772, -0.006465390007403401, 0.003861941718706894, -0.006234826197601447, -0.01584484837082699, -0.012674598314356609, -0.008460406408284722, -0.0026690948901822334, 0.0009911036113632754, 0.0126681936088639, 0.022006022317907475, -0.010958179151827369, -0.015588666670376795, 0.0343027532527428, -0.0017164185804526167, -0.011906052281683427, -0.012258302701879067, 0.018009584810652124, 0.0036826143886933686, -0.00014640394119542376, 0.007839164876653468, -0.16262424365609515, 0.007877592178287127, 0.017048903201133243, -0.011406497360445859, 0.0030405584955334056, -0.016036985810317882, 0.019239258462929214, -0.014896975240970932, -0.023299741162466463, 0.006116341939954115, 0.008351529208876454, -0.015473384765475818, -0.025554141616544335, -0.014115620728634929, 0.009216143029972484, -0.027078424270905278, -0.013206175831735126, 0.027411459643300193, 0.012085379378866303, 0.004150146248128688, 0.012322347894160967, -0.023594350164550318, 0.005623192189870555, -0.01211099820083714, 0.022851422953847973, 0.008185010591356399, -0.007704669786596959, 0.02738584268397455, 0.008409169695665644, -0.026309878238232096, 0.011163125070981083, 0.010125587926872287, 0.03253509924023967, -0.01785587560411749, 0.016126647961925426, -0.012405607202920995, -0.012681002088526721, -0.002515385683647597, -0.0021038936002895883, 0.02331255057345188, 0.026335497060202932, 0.01478169426739255, 0.00902400652180419, 0.013180557009764289, -0.00956839251884553, -0.003692221214101783, 0.02055859426681385, 0.010631547553602565, -0.01752284023172257, -0.027360223862003714, 0.02761640463113131, -0.03046002495202208, -0.0012320747183449088, -0.007845569582146177, 0.01695923918688051, 0.004230203204142361, 0.039195825314589934, 0.01982847646709692, -0.020135894880166195, -0.028974168667972203, 0.0024177164860209213, -0.028308096060537177, 0.0031910653493216873, 0.0058185305851239065, -0.024631886377336515, -0.0014698434725801874, 0.019687576671547705, 0.007851974287638886, -0.029153496696477675, 0.008287483085271958, -0.020917250323824795, -0.011995716295936163, -0.006436569298347509, -0.012956397905455044, -0.00960041511498648, 0.013462357532185321, -0.016639013225471008, -0.005661619491504214, -0.03361105996069174, 0.0035257030622430265, -0.007819951691497936, 0.04011808055379775, 0.0017324298785230917, -0.003788289468185931, -0.013347075627284344, 0.01861161222580525, -0.003098199447983894, 0.000970288842380931, -0.007999278788680814, -0.019700386082533123, 0.022249395538694847, -0.014845739459674451, -0.01563990338299587, -0.014756075445421714, 0.020456122704220887, 0.009350638120028993, 0.03263757080283263, -0.010573907066813375, -0.0011584223514086208, -0.02964024313805242, -0.004393519003254763, 0.013987530344071129, -0.00026058499368409487, 0.008767824821353994, 0.0022511981013315155, -0.0046048691622398875, 0.03630096921240268, 0.0002968107069084663, 0.017740594630539105, -0.014896975240970932, -0.015076303269476406, -0.014192475331902247, 0.017087331434089498, 0.008261865194623718, -0.014384611840070543, 0.03338049801353498, 0.011028629049601977, 0.006017071798784911, 0.008582093018678408, 0.019418586491434686, 0.04739364717957695, -0.0054406622742800255, -0.024964923612376625, -0.0053509987256885865, -0.004095707648424554, -0.017471602587780896, -0.11374475323098467, -0.03542995161713654, 0.02377367819305579, 0.011143910954502956, 0.010650761670080692, -0.006500614956290706, -0.010189634050476784, -0.02519548742217858, 0.005751283040095653, 0.016562158622203692, -0.0074164650243445165, -0.02738584268397455, -0.012674598314356609, -0.015627093972010453, -0.018265767442424915, -0.009574797224338238, 0.03594231688068212, -0.012841116000554067, -0.02171141331582362, 0.020263985264729993, 0.007141069673077492, 0.0071026423714438324, -0.005527123935786407, 0.0074612970314708845, -0.029512150890843428, -0.002116702778444358, -0.017241040640624136, 0.005187682926576185, 0.005219705522717135, -0.005037176072787903, -0.005748080687349298, -0.02274894952860982, 0.0023344571772608937, -0.005555944179181003, 0.006936124219585076, 0.013475166011848142, -0.003817109944411175, -0.011220765557770273, -0.0037210416903270273, -0.020942867283150438, 0.00039247863779548844, 0.005658417138757859, 0.005149255624942526, -0.015153157872743723, 0.012616956896244822, -0.00026158569981349957, -0.030511260733318564, 0.028256860279240695, 0.0011047843398910755, 0.00430065310191697, -0.03794053656563239, 0.017932730207384805, -0.0038843574894394293, -0.010477838347067928, 0.013961911522100292, -0.012194256578274571, -0.011092675173206473, -0.006801628663867269, -0.013692921341987276, 0.006247635142925568, 0.005738474094771532, -0.019687576671547705, -0.014743266965758893, 0.030562496514615047, 0.0018333015453114467, -0.007608601532512811, -0.02887169710537924, -0.01311651181748239, 0.015486194176461236, -0.01759969483498989, -0.015883276603783244, 0.0002838014690185431, -0.005088412319745683, 0.01739474984715877, -0.02350468615029758, -0.0009542774861027938, -0.014627985060857916, -0.008268269900116428, -0.007538151634738203, -0.020635448870081168, -0.030383170348754766, -0.02003342145492804, 0.006942528459416487, -0.017868685015102907, 0.009638842416620138, -0.0013457553304573842, -0.0026226619395133366, 0.0020910844221348193, 0.01610103100259978, -0.017612504245975307, -0.00860130620383394, 0.024759976761900317, 0.0224159141562149, 0.00029440900055636255, -0.04147584459063864, -0.0019181617976140022, -0.0008089742809043516, -0.020571403677799267, 0.021122195311655913, 0.011131102474840133, 0.025925605221895506, -0.01501225714587191, -0.003913178198495322, 0.015563048779728554, -0.005975442144404898, 0.019252067873914633, 0.024388513156549142, -0.007871187472794418, 0.015934512385079726, -0.008953556624029581, -0.010599524957461614, -0.010394579038307901, -0.026015269236148244, 0.010471433641575219, -0.0021407197255500702, -0.01352640272446722, -0.020507358485517366, -0.02062264132174094, 0.0031766552276243894, 0.013308648325650685, 0.0012056558902641058, 0.005168469275759356, 0.009120074310227039, -0.0035737371892851006, 0.0028772426964210037, 0.026489204404092376, 0.009235356215128016, 0.009786147848984662, -0.016856767624287543, 0.002497773209203945, -0.02697595084566712, -0.012802688698920407, 0.031715315563624814, -0.002760359615146849, -4.147944586959823e-05, 0.025016159393673108, -0.013154939119116048, -0.023940194947930652, -0.0026338699412949288, 0.008409169695665644, -0.0024993743855771223, 0.025669422590122715, 0.00013089292343007337, -0.03135665950661387, 0.011803579787767868, -0.02577189601536087, 0.001244083191897765, 0.013718539232635515, -0.022364676512273227, 0.015550239368743135, 0.02041769447126463, 0.013154939119116048, 0.009773338437999244, 0.01971319549351854, -0.006801628663867269, -0.012014929481091695, -0.02321007714821373, -0.03673648173532614, 0.00862692409448218, 0.001072761627334801, 0.01134245216816396, -0.02666853243259785, 0.019316113066196534, 0.013897866329818393, 0.020648258281066586, 0.0031270199242091385, 0.013001229912581413, 0.0030245471974629304, -0.001099980927186868, 0.0008301893439799905, -0.009209738324479775, -0.03819671733475999, -0.010285701838899633, -0.0009422689543422754, 0.020776350528275575, 0.01079806617112262, 0.03893964454546234, -0.015511812067109478, 0.00901760181631148, -0.011528183970839545, -0.03281689883133811, 0.04311540821857797, 0.0031382279259907302, 0.01628035716846006, -0.01549900265612406, 0.0010007104367716904, 0.023261312929510208, 0.004275034745607431, -0.010766042643659074, 0.010567502361320666, 0.008018492905158941, 0.0054566740380117985, -0.005796114581560723, 0.000182429507373148, -0.012219875400245408, -0.025682232001108134, 0.009280188222254385, 0.01598574816637621, -0.006401344349460204, -0.006129151350939534, 0.01651092097826202, 0.03835042654129463, 0.00499554641840789, -0.0063116808008687655, 0.006654324162825342, -0.0008758217646699606, 0.004194978255255057, 0.0025906391105417377, -0.006372524106065609, -0.01706171261211866, -0.015050684447505569, 0.008895915205917794, 0.015537429957757717, 0.012828307520891244, 0.0016347605644810913, -0.002808393742188923, -0.02356873134257948, 0.023517495561283, 0.025016159393673108, -0.030075751935685492, -0.03453331519989956, 0.0015418946631432981, -0.003637782614397649, 0.005642405840687385, -0.006673537813642172, -0.02255681395176412, 0.018201722250143014, 0.0034872757606093673, 0.013859439028184734, -0.026361114019528578, 0.020456122704220887, -0.014692030253139815, 0.0008942348273002015, -0.004992344065661535, -0.01497382984423825, -0.021480849506021665, -0.0069681468157260255, 0.008101751282596372, -0.011169529776473792, 0.03166407791968314, -0.013001229912581413, 0.05482291928660039, -0.013372693517932584, -0.01079166146562991, -0.00024657504966476705, -0.007294778879612128, 0.01867565741808715, 0.012745048212131217, 0.014986639255223668, -0.01758688542400447, 0.01795834902935564, 0.007089833426119713, -0.009158501611860699, 0.02484964077615305, -0.008562878902200281, 0.0042366074439737725, 0.011617847985092283, -0.005194087632068894, 0.02828247910121153, 0.01759969483498989, -0.008357933914369163, 0.05256851883252252, 0.0034392416335672937, 0.03691580603854123, -0.008325910386905618, -0.019687576671547705, -0.014333375127451465, 0.019866904700053176, 0.006974551521218735, -0.015627093972010453, -0.02623302363496478, 0.014025957645704789, 0.014602366238887079, -0.019854095289067758, -0.023735249960099535, 0.009901428822563042, -0.017369131025187935, 0.00404126904872042, -0.004387114297762054, -0.0052677398825898576, 0.031715315563624814, 0.013180557009764289, 0.04142460694669697, -0.010266488653744101, -0.0015090713624004352, 0.002139118549176893, 0.023363786354748364, 0.005738474094771532, 0.005523921583040052, -0.020789158076615803]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Turn the first text chunk into a vector with the embedding\n",
    "\n",
    "query_result = embeddings.embed_query(texts[0].page_content)\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an agent to solve a simple quadratic equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python REPL tool and instantiate Python agent\n",
    "\n",
    "from langchain.agents.agent_toolkits import create_python_agent\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "from langchain.python import PythonREPL\n",
    "from langchain.llms.openai import OpenAI\n",
    "\n",
    "agent_executor = create_python_agent(\n",
    "    llm=OpenAI(model_name=\"gpt-3.5-turbo-1106\" ,temperature=0, max_tokens=1000),\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI can use the quadratic formula to find the roots of the quadratic function.\n",
      "Action: Python_REPL\n",
      "Action Input: import cmath\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to define the coefficients of the quadratic function and then use the quadratic formula to find the roots.\n",
      "Action: Python_REPL\n",
      "Action Input: a = 3, b = 2, c = -1\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mSyntaxError('cannot assign to literal', ('<string>', 1, 5, 'a = 3, b = 2, c = -1\\n'))\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to define the coefficients separately.\n",
      "Action: Python_REPL\n",
      "Action Input: a = 3\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to define the other coefficients separately as well.\n",
      "Action: Python_REPL\n",
      "Action Input: b = 2\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI can now use the quadratic formula to find the roots.\n",
      "Action: Python_REPL\n",
      "Action Input: root1 = (-b + cmath.sqrt(b**2 - 4*a*c)) / (2*a), root2 = (-b - cmath.sqrt(b**2 - 4*a*c)) / (2*a)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mSyntaxError('cannot assign to operator', ('<string>', 1, 9, 'root1 = (-b + cmath.sqrt(b**2 - 4*a*c)) / (2*a), root2 = (-b - cmath.sqrt(b**2 - 4*a*c)) / (2*a)\\n'))\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI made a mistake in trying to assign the roots in a single line. I should assign them separately.\n",
      "Action: Python_REPL\n",
      "Action Input: root1 = (-b + cmath.sqrt(b**2 - 4*a*c)) / (2*a)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to define the value of c.\n",
      "Action: Python_REPL\n",
      "Action Input: c = -1\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: root1 = (0.3333333333333333+0j), root2 = (-1+0j)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'root1 = (0.3333333333333333+0j), root2 = (-1+0j)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the Python agent\n",
    "\n",
    "agent_executor.run(\"Find the roots (zeros) if the quadratic function 3 * x**2 + 2*x -1\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCEL example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream go to therapy?\\n\\nBecause it had a rocky road!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "import openai\n",
    "\n",
    "\n",
    "prompt_template = \"Tell me a short joke about {topic}\"\n",
    "client = openai.OpenAI()\n",
    "\n",
    "def call_chat_model(messages: List[dict]) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def invoke_chain(topic: str) -> str:\n",
    "    prompt_value = prompt_template.format(topic=topic)\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_value}]\n",
    "    return call_chat_model(messages)\n",
    "\n",
    "invoke_chain(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream go to therapy? \\n\\nBecause it had a rocky road!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks using multiple handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in MyCustomHandlerOne.on_chain_start callback: KeyError('name')\n",
      "Error in MyCustomHandlerOne.on_chain_start callback: KeyError('name')\n",
      "Error in MyCustomHandlerOne.on_llm_start callback: KeyError('name')\n",
      "Error in MyCustomHandlerTwo.on_llm_start callback: KeyError('second handler')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_new_token  I\n",
      "on_new_token  should\n",
      "on_new_token  use\n",
      "on_new_token  a\n",
      "on_new_token  calculator\n",
      "on_new_token  to\n",
      "on_new_token  solve\n",
      "on_new_token  this\n",
      "on_new_token .\n",
      "Action\n",
      "on_new_token :\n",
      "on_new_token  Calculator\n",
      "on_new_token \n",
      "Action\n",
      "on_new_token  Input\n",
      "on_new_token :\n",
      "on_new_token  \n",
      "on_new_token 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in MyCustomHandlerOne.on_chain_start callback: KeyError('name')\n",
      "Error in MyCustomHandlerOne.on_chain_start callback: KeyError('name')\n",
      "Error in MyCustomHandlerOne.on_llm_start callback: KeyError('name')\n",
      "Error in MyCustomHandlerTwo.on_llm_start callback: KeyError('second handler')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_new_token  ^\n",
      "on_new_token  \n",
      "on_new_token 0\n",
      "on_new_token .\n",
      "on_new_token 235\n",
      "on_new_token \n",
      "on_agent_action tool='Calculator' tool_input='2 ^ 0.235' log=' I should use a calculator to solve this.\\nAction: Calculator\\nAction Input: 2 ^ 0.235'\n",
      "on_tool_start Calculator\n",
      "on_new_token ```text\n",
      "on_new_token \n",
      "\n",
      "on_new_token 2\n",
      "on_new_token  **\n",
      "on_new_token  \n",
      "on_new_token 0\n",
      "on_new_token .\n",
      "on_new_token 235\n",
      "on_new_token \n",
      "\n",
      "on_new_token ```\n",
      "\n",
      "on_new_token ...\n",
      "on_new_token num\n",
      "on_new_token expr\n",
      "on_new_token .evaluate\n",
      "on_new_token (\"\n",
      "on_new_token 2\n",
      "on_new_token  **\n",
      "on_new_token  \n",
      "on_new_token 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in MyCustomHandlerOne.on_chain_start callback: KeyError('name')\n",
      "Error in MyCustomHandlerOne.on_llm_start callback: KeyError('name')\n",
      "Error in MyCustomHandlerTwo.on_llm_start callback: KeyError('second handler')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_new_token .\n",
      "on_new_token 235\n",
      "on_new_token \")\n",
      "on_new_token ...\n",
      "\n",
      "on_new_token \n",
      "on_new_token  I\n",
      "on_new_token  now\n",
      "on_new_token  know\n",
      "on_new_token  the\n",
      "on_new_token  final\n",
      "on_new_token  answer\n",
      "on_new_token .\n",
      "Final Answer:\n",
      "on_new_token  \n",
      "on_new_token 1.\n",
      "on_new_token 1769067372187674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.1769067372187674'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.schema import AgentAction\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "\n",
    "# First, define custom callback handler implementations\n",
    "class MyCustomHandlerOne(BaseCallbackHandler):\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"on_llm_start {serialized['name']}\")\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:\n",
    "        print(f\"on_new_token {token}\")\n",
    "\n",
    "    def on_llm_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when LLM errors.\"\"\"\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"on_chain_start {serialized['name']}\")\n",
    "\n",
    "    def on_tool_start(\n",
    "        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"on_tool_start {serialized['name']}\")\n",
    "\n",
    "    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n",
    "        print(f\"on_agent_action {action}\")\n",
    "\n",
    "\n",
    "class MyCustomHandlerTwo(BaseCallbackHandler):\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"on_llm_start (I'm the second handler!!) {serialized['second handler']}\")\n",
    "\n",
    "\n",
    "# Instantiate the handlers\n",
    "handler1 = MyCustomHandlerOne()\n",
    "handler2 = MyCustomHandlerTwo()\n",
    "\n",
    "# Setup the agent. Only the `llm` will issue callbacks for handler2\n",
    "llm = OpenAI(temperature=0, streaming=True, callbacks=[handler2])\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)\n",
    "\n",
    "# Callbacks for handler1 will be issued by every object involved in the\n",
    "# Agent execution (llm, llmchain, tool, agent executor)\n",
    "agent.run(\"What is 2 raised to the 0.235 power?\", callbacks=[handler1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
